{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob \n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import h5py\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from math import log10, sqrt\n",
    "from skimage.measure import compare_ssim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_frames = []\n",
    "\n",
    "with open('good_frames.txt','r') as good_frames_file:\n",
    "    for frame in good_frames_file:\n",
    "        good_frames.append(frame.replace('\\n',''))\n",
    "\n",
    "parts = set()\n",
    "\n",
    "video_dict = {}\n",
    "\n",
    "for frame in good_frames:\n",
    "    if ('fakes_low_quality' in frame):\n",
    "        continue\n",
    "    folder_base_name = os.path.dirname(frame)\n",
    "    hdf5_base_name = os.path.basename(frame).split('_')[0] + '.h5'\n",
    "    hdf5_key = os.path.join(folder_base_name, hdf5_base_name)\n",
    "    \n",
    "    current_frames = video_dict.get(hdf5_key,[])\n",
    "    video_dict[hdf5_key] = current_frames\n",
    "    current_frames.append(frame)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakes = []\n",
    "originals = []\n",
    "labels = []\n",
    "\n",
    "for key in video_dict:\n",
    "    if ('fakes_low_quality' in key):\n",
    "        continue\n",
    "    if 'fakes' in key:\n",
    "        label = 0\n",
    "        fakes.append(key)\n",
    "    elif 'originals' in key:\n",
    "        label = 1\n",
    "        originals.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\silbe\\desktop\\deepfa~1\\deepfa~2\\venv\\lib\\site-packages\\ipykernel_launcher.py:13: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  del sys.path[0]\n",
      "c:\\users\\silbe\\desktop\\deepfa~1\\deepfa~2\\venv\\lib\\site-packages\\ipykernel_launcher.py:14: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "original_labels = [1 for i in range(len(originals))]\n",
    "fakes_labels = [0 for i in range(len(fakes))]\n",
    "\n",
    "x_train_originals, x_test_originals, y_train_originals, y_test_originals = train_test_split(originals,original_labels,train_size=0.8,test_size=0.2, random_state=42)\n",
    "x_train_fakes, x_test_fakes, y_train_fakes, y_test_fakes = train_test_split(fakes,fakes_labels,train_size=0.8,test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = x_train_originals + x_train_fakes\n",
    "x_test = x_test_originals + x_test_fakes\n",
    "\n",
    "y_train = y_train_originals + y_train_fakes\n",
    "y_test = y_train_fakes + y_test_fakes\n",
    "\n",
    "training_set = np.vstack([h5py.File(x,'r').get(x).value for x in x_train])\n",
    "testing_set = np.vstack([h5py.File(x,'r').get(x).value for x in x_test])\n",
    "\n",
    "training_data = training_set[:,0:259]\n",
    "testing_data = testing_set[:,0:259]\n",
    "training_labels = training_set[:,259:260]\n",
    "testing_labels = testing_set[:,259:260]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "training_data = scaler.fit_transform( training_data )\n",
    "testing_data = scaler.transform( testing_data )\n",
    "\n",
    "training_set = np.hstack([training_data,training_labels])\n",
    "testing_set = np.hstack([testing_data, testing_labels])\n",
    "\n",
    "training_labels = training_set[:,259:260].ravel()\n",
    "testing_labels = testing_set[:,259:260].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 3 SVM models (3 kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs = svm.SVC(kernel='linear')\n",
    "clfs.fit(training_data, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfr = svm.SVC(kernel='rbf')\n",
    "clfr.fit(training_data, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='sigmoid')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfsig = svm.SVC(kernel='sigmoid')\n",
    "clfsig.fit(training_data, training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models for evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_linear = \"svm_model_linear.pkl\"\n",
    "svm_rbf =  \"svm_model_rbf.pkl\"\n",
    "svm_sig =  \"svm_model_sig.pkl\"\n",
    "\n",
    "training_filename = \"training_set.pkl\"\n",
    "testing_filename = \"testing_set.pkl\"\n",
    "testing_associated_files = \"testing_files.pkl\"\n",
    "\n",
    "with open(svm_linear, 'wb') as model_file:\n",
    "    pickle.dump(clfs, model_file)\n",
    "    \n",
    "with open(svm_rbf, 'wb') as model_file:\n",
    "    pickle.dump(clfr, model_file)\n",
    "    \n",
    "with open(svm_sig, 'wb') as model_file:\n",
    "    pickle.dump(clfsig, model_file)\n",
    "\n",
    "with open(training_filename, 'wb') as training_file:\n",
    "    pickle.dump(training_set, training_file)\n",
    "    \n",
    "with open(testing_filename, 'wb') as testing_file:\n",
    "    pickle.dump(testing_set, testing_file)\n",
    "    \n",
    "with open(testing_associated_files, 'wb') as testing_associated_file:\n",
    "    pickle.dump(x_test, testing_associated_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done Training Data..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
